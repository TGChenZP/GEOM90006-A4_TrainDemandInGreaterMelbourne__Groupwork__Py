{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "py_file_location = '../'\n",
    "home_directory = '../'\n",
    "\n",
    "sys.path.append(os.path.abspath(py_file_location))\n",
    "from model.model_class.environment import *\n",
    "\n",
    "from model.model_class import GNN\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_train_data = pd.read_parquet('../data/curated/ML_data/gnn_train_data.parquet')\n",
    "gnn_val_data = pd.read_parquet('../data/curated/ML_data/gnn_val_data.parquet')\n",
    "gnn_test_data = pd.read_parquet('../data/curated/ML_data/gnn_test_data.parquet')\n",
    "\n",
    "SA2_gnn_data = pd.read_parquet('../data/curated/ML_data/SA2_gnn_data.parquet')\n",
    "SA2_gnn_data['Station Name'] = SA2_gnn_data['Station Name'].apply(lambda x: '(SA2)'+x)\n",
    "station_inference_gnn_data = pd.read_parquet('../data/curated/ML_data/station_inference_gnn_data.parquet')\n",
    "\n",
    "station_inference_gnn_data = station_inference_gnn_data.rename({'Station_Name': 'Station Name'}, axis=1)\n",
    "inference_data = pd.concat([SA2_gnn_data, station_inference_gnn_data], axis=0)\n",
    "inference_data.drop(columns=['Station_Na'], axis=1, inplace=True)\n",
    "inference_data = inference_data.rename({'Station Name': 'Station_Name'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open npy\n",
    "station_weights_matrix = np.load('../data/curated/ML_data/station_weights_matrix.npy')\n",
    "SA2_weights_matrix = np.load('../data/curated/ML_data/station_weights_withSA2_matrix.npy')\n",
    "\n",
    "with open('../data/curated/ML_features/station_weights_withSA2.json', 'r') as f:\n",
    "    station_weights_withSA2 = json.load(f)\n",
    "\n",
    "with open('../data/curated/ML_features/station_weights.json', 'r') as f:\n",
    "    station_weights = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit SA2_weights_matrix matrix: columns after 223 masked out to 0\n",
    "SA2_weights_matrix[:, 223:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "geospatial_features = ['log_Total_Demand']\n",
    "non_geospatial_features = ['Weekday', 'mean_rainfall_value', 'has_school',\n",
    "       'has_sport_facility', 'has_shopping_centre', 'has_hospital',\n",
    "       'total_population', ' med_rent_weekly_c2021',\n",
    "       ' med_mortg_rep_mon_c2021', ' med_person_inc_we_c2021',\n",
    "       ' med_famly_inc_we_c2021']\n",
    "label_columns = ['log_Total_Demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataFactory(raw_dataset, geospatial_features, non_geospatial_features, label_columns, stations_index, inference = False):\n",
    "\n",
    "    \"\"\" Data Factory of GNN \"\"\"\n",
    "    \n",
    "    geospatial_x_batches = []\n",
    "    non_geospatial_x_batches = []\n",
    "    y_batches = []\n",
    "    masks = []\n",
    "\n",
    "    if inference:\n",
    "        groupby_column = 'Weekday'\n",
    "    else:\n",
    "        groupby_column = 'Business_Date'\n",
    "\n",
    "    for day, daily_df in tqdm(raw_dataset.groupby(groupby_column)):\n",
    "\n",
    "        geospatial_x = np.zeros([len(stations_index), len(geospatial_features)])\n",
    "        y = np.zeros([len(stations_index), len(label_columns)])\n",
    "        mask = np.zeros([len(stations_index), 1])\n",
    "        non_geospatial_x = np.zeros([len(stations_index), len(non_geospatial_features)])\n",
    "\n",
    "        daily_df.set_index('Station_Name', inplace=True)\n",
    "\n",
    "        for station in daily_df.index:\n",
    "\n",
    "            geospatial_x[stations_index[station]] = daily_df.loc[station][geospatial_features] # todo inference. \n",
    "            if not inference:\n",
    "                y[stations_index[station]] = daily_df.loc[station][label_columns]\n",
    "            mask[stations_index[station]] = 1\n",
    "            non_geospatial_x[stations_index[station]] = daily_df.loc[station][non_geospatial_features]\n",
    "                \n",
    "        geospatial_x = np.nan_to_num(geospatial_x)\n",
    "        geospatial_x_batches.append(geospatial_x)\n",
    "        y_batches.append(y)\n",
    "        masks.append(mask.flatten())\n",
    "\n",
    "        non_geospatial_x_batches.append(non_geospatial_x)\n",
    "\n",
    "        \n",
    "    return geospatial_x_batches, non_geospatial_x_batches, y_batches, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/382 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [01:11<00:00,  5.34it/s]\n",
      "100%|██████████| 82/82 [00:15<00:00,  5.42it/s]\n",
      "100%|██████████| 82/82 [00:14<00:00,  5.59it/s]\n"
     ]
    }
   ],
   "source": [
    "train_geospatial_X_batches, train_non_geospatial_X_batches, train_y_batches, train_masks = DataFactory(gnn_train_data, geospatial_features, non_geospatial_features, label_columns, station_weights)\n",
    "val_geospatial_X_batches, val_non_geospatial_X_batches, val_y_batches, val_masks = DataFactory(gnn_val_data, geospatial_features, non_geospatial_features, label_columns, station_weights)\n",
    "test_geospatial_X_batches, test_non_geospatial_X_batches, test_y_batches, test_masks = DataFactory(gnn_test_data, geospatial_features, non_geospatial_features, label_columns, station_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "all_inference_geospatial_X_batches, all_inference_non_geospatial_X_batches, all_inference_y_batches, all_inference_masks = DataFactory(inference_data, geospatial_features, non_geospatial_features, label_columns, station_weights_withSA2, inference = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/382 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:23<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1 Train | Loss:  0.1195 | R2:  0.8701| MSE:  0.1191 | RMSE:  0.3452 | MAE:  0.2217 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 63.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Val | Loss:  0.0248 | R2:  0.9751| MSE:  0.0248 | RMSE:  0.1576 | MAE:  0.1200 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:27<00:00, 14.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2 Train | Loss:  0.0350 | R2:  0.9617| MSE:  0.0348 | RMSE:  0.1866 | MAE:  0.1311 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 52.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Val | Loss:  0.0204 | R2:  0.9796| MSE:  0.0204 | RMSE:  0.1428 | MAE:  0.0976 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:27<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3 Train | Loss:  0.0253 | R2:  0.9722| MSE:  0.0254 | RMSE:  0.1593 | MAE:  0.1069 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:02<00:00, 40.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Val | Loss:  0.0078 | R2:  0.9922| MSE:  0.0078 | RMSE:  0.0884 | MAE:  0.0666 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:27<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 4 Train | Loss:  0.0211 | R2:  0.9768| MSE:  0.0211 | RMSE:  0.1453 | MAE:  0.0951 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 50.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Val | Loss:  0.0182 | R2:  0.9818| MSE:  0.0182 | RMSE:  0.1348 | MAE:  0.1128 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:27<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 5 Train | Loss:  0.0191 | R2:  0.9788| MSE:  0.0192 | RMSE:  0.1387 | MAE:  0.0897 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 55.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Val | Loss:  0.0077 | R2:  0.9923| MSE:  0.0077 | RMSE:  0.0875 | MAE:  0.0636 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:25<00:00, 15.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 6 Train | Loss:  0.0180 | R2:  0.9801| MSE:  0.0180 | RMSE:  0.1343 | MAE:  0.0859 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 49.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Val | Loss:  0.0072 | R2:  0.9928| MSE:  0.0072 | RMSE:  0.0848 | MAE:  0.0589 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:25<00:00, 15.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 7 Train | Loss:  0.0241 | R2:  0.9733| MSE:  0.0242 | RMSE:  0.1554 | MAE:  0.1012 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 55.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Val | Loss:  0.0091 | R2:  0.9909| MSE:  0.0091 | RMSE:  0.0954 | MAE:  0.0670 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:25<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 8 Train | Loss:  0.0181 | R2:  0.9801| MSE:  0.0182 | RMSE:  0.1348 | MAE:  0.0870 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 44.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Val | Loss:  0.0109 | R2:  0.9890| MSE:  0.0109 | RMSE:  0.1045 | MAE:  0.0707 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:27<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 9 Train | Loss:  0.0173 | R2:  0.9809| MSE:  0.0174 | RMSE:  0.1318 | MAE:  0.0841 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 52.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Val | Loss:  0.0094 | R2:  0.9906| MSE:  0.0094 | RMSE:  0.0967 | MAE:  0.0719 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:24<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 10 Train | Loss:  0.0156 | R2:  0.9829| MSE:  0.0156 | RMSE:  0.1250 | MAE:  0.0767 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 57.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Val | Loss:  0.0071 | R2:  0.9929| MSE:  0.0071 | RMSE:  0.0841 | MAE:  0.0635 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:26<00:00, 14.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 11 Train | Loss:  0.0142 | R2:  0.9843| MSE:  0.0143 | RMSE:  0.1194 | MAE:  0.0741 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 51.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Val | Loss:  0.0051 | R2:  0.9948| MSE:  0.0051 | RMSE:  0.0717 | MAE:  0.0536 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:24<00:00, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 12 Train | Loss:  0.0141 | R2:  0.9845| MSE:  0.0142 | RMSE:  0.1190 | MAE:  0.0726 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 56.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Val | Loss:  0.0047 | R2:  0.9952| MSE:  0.0047 | RMSE:  0.0689 | MAE:  0.0515 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:25<00:00, 15.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 13 Train | Loss:  0.0139 | R2:  0.9846| MSE:  0.0140 | RMSE:  0.1183 | MAE:  0.0716 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 54.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Val | Loss:  0.0044 | R2:  0.9956| MSE:  0.0044 | RMSE:  0.0663 | MAE:  0.0538 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [00:24<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 14 Train | Loss:  0.0144 | R2:  0.9842| MSE:  0.0145 | RMSE:  0.1202 | MAE:  0.0739 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 50.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Val | Loss:  0.0057 | R2:  0.9943| MSE:  0.0057 | RMSE:  0.0753 | MAE:  0.0541 \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 53.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Val | Loss:  0.0044 | R2:  0.9956| MSE:  0.0044 | RMSE:  0.0663 | MAE:  0.0538 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:01<00:00, 56.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Val | Loss:  0.0043 | R2:  0.9952| MSE:  0.0043 | RMSE:  0.0659 | MAE:  0.0533 \n"
     ]
    }
   ],
   "source": [
    "class GNN_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 256\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    n_gnn_layers = 2\n",
    "    activation = nn.ReLU()\n",
    "    res_learning = False\n",
    "    bottleneck = True\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    epochs = 32\n",
    "    lr = 1e-3\n",
    "    patience = 5\n",
    "    loss = nn.MSELoss()\n",
    "    validation_loss = nn.MSELoss()\n",
    "    alpha = 0.1\n",
    "    scheduler = True\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    spatial_input_dim = 1\n",
    "    nonspatial_input_dim = 11\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory\n",
    "    name = f'AGNN_2layer'\n",
    "\n",
    "model2 = GNN(GNN_config) # initialise the model\n",
    "\n",
    "# train the model (all cells except this one will print training log and evaluation at each batch)\n",
    "best_epoch = model2.fit(train_geospatial_X_batches, train_non_geospatial_X_batches, train_y_batches, train_masks, val_geospatial_X_batches, val_non_geospatial_X_batches, val_y_batches, val_masks, station_weights_matrix)\n",
    "print('\\n\\n')\n",
    "\n",
    "# as model automatically saves best epoch, will now load the best epoch and evaluate on test set\n",
    "model2.load()\n",
    "model2.eval(val_geospatial_X_batches, val_non_geospatial_X_batches, val_y_batches, val_masks, station_weights_matrix, best_epoch, evaluation_mode = True)\n",
    "model2.eval(test_geospatial_X_batches, test_non_geospatial_X_batches, test_y_batches, test_masks, station_weights_matrix, best_epoch, evaluation_mode = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../output', exist_ok=True)\n",
    "\n",
    "# read in y scale\n",
    "with open('../data/curated/ML_data/y_scaler_gnn.pickle', 'rb') as f:\n",
    "    y_scaler_gnn = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 29.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station Name</th>\n",
       "      <th>Predicted_Log_Total_Demand</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Unscaled_Predicted_Log_Total_Demand</th>\n",
       "      <th>Unscaled_Predicted_Total_Demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aircraft</td>\n",
       "      <td>2.885838</td>\n",
       "      <td>0</td>\n",
       "      <td>12.258663</td>\n",
       "      <td>210799.561393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alamein</td>\n",
       "      <td>2.880375</td>\n",
       "      <td>0</td>\n",
       "      <td>12.248887</td>\n",
       "      <td>208748.728791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albion</td>\n",
       "      <td>2.902173</td>\n",
       "      <td>0</td>\n",
       "      <td>12.287896</td>\n",
       "      <td>217052.903606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alphington</td>\n",
       "      <td>2.916082</td>\n",
       "      <td>0</td>\n",
       "      <td>12.312788</td>\n",
       "      <td>222523.576722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Altona</td>\n",
       "      <td>2.900282</td>\n",
       "      <td>0</td>\n",
       "      <td>12.284512</td>\n",
       "      <td>216319.642428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>(SA2)Whittlesea</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>1</td>\n",
       "      <td>7.128467</td>\n",
       "      <td>1246.964479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>(SA2)Wollert</td>\n",
       "      <td>-0.413111</td>\n",
       "      <td>1</td>\n",
       "      <td>6.354812</td>\n",
       "      <td>575.254399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>(SA2)Wyndham Vale - North</td>\n",
       "      <td>0.147060</td>\n",
       "      <td>1</td>\n",
       "      <td>7.357303</td>\n",
       "      <td>1567.602840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>(SA2)Wyndham Vale - South</td>\n",
       "      <td>0.052260</td>\n",
       "      <td>1</td>\n",
       "      <td>7.187648</td>\n",
       "      <td>1322.987988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>(SA2)Yarra Valley</td>\n",
       "      <td>-0.044684</td>\n",
       "      <td>1</td>\n",
       "      <td>7.014156</td>\n",
       "      <td>1112.267860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>884 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Station Name  Predicted_Log_Total_Demand  Weekday  \\\n",
       "0                     Aircraft                    2.885838        0   \n",
       "1                      Alamein                    2.880375        0   \n",
       "2                       Albion                    2.902173        0   \n",
       "3                   Alphington                    2.916082        0   \n",
       "4                       Altona                    2.900282        0   \n",
       "..                         ...                         ...      ...   \n",
       "879            (SA2)Whittlesea                    0.019191        1   \n",
       "880               (SA2)Wollert                   -0.413111        1   \n",
       "881  (SA2)Wyndham Vale - North                    0.147060        1   \n",
       "882  (SA2)Wyndham Vale - South                    0.052260        1   \n",
       "883          (SA2)Yarra Valley                   -0.044684        1   \n",
       "\n",
       "     Unscaled_Predicted_Log_Total_Demand  Unscaled_Predicted_Total_Demand  \n",
       "0                              12.258663                    210799.561393  \n",
       "1                              12.248887                    208748.728791  \n",
       "2                              12.287896                    217052.903606  \n",
       "3                              12.312788                    222523.576722  \n",
       "4                              12.284512                    216319.642428  \n",
       "..                                   ...                              ...  \n",
       "879                             7.128467                      1246.964479  \n",
       "880                             6.354812                       575.254399  \n",
       "881                             7.357303                      1567.602840  \n",
       "882                             7.187648                      1322.987988  \n",
       "883                             7.014156                      1112.267860  \n",
       "\n",
       "[884 rows x 5 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions = model2.predict(all_inference_geospatial_X_batches, all_inference_non_geospatial_X_batches, all_inference_masks, SA2_weights_matrix)\n",
    "all_predictions = np.array(all_predictions).flatten()\n",
    "\n",
    "all_predictions_df = pd.DataFrame({'Station Name': list(station_weights_withSA2.keys()) * 2,\n",
    "              'Predicted_Log_Total_Demand': all_predictions,\n",
    "              'Weekday': [0 for _ in range(len(station_weights_withSA2))] + [1 for _ in range(len(station_weights_withSA2))]})\n",
    "all_predictions_df['Unscaled_Predicted_Log_Total_Demand'] = y_scaler_gnn.inverse_transform(all_predictions_df['Predicted_Log_Total_Demand'].values.reshape(-1, 1))\n",
    "all_predictions_df['Unscaled_Predicted_Total_Demand'] = np.exp(all_predictions_df['Unscaled_Predicted_Log_Total_Demand'])\n",
    "all_predictions_df.to_csv('../output/agnn2_predictions.csv', index=False)\n",
    "\n",
    "all_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_25905/2378514990.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  weekday_weekend_scaled_predictions = all_predictions_df[['Station Name', 'Weekday', 'Unscaled_Predicted_Total_Demand']].groupby('Station Name').apply(lambda x: 5/7 * x[x['Weekday'] == 1]['Unscaled_Predicted_Total_Demand'].values[0] + 2/7 * x[x['Weekday'] == 0]['Unscaled_Predicted_Total_Demand'].values[0]).reset_index().rename({0: 'Predicted_Total_Demand'}, axis=1).loc[:218]\n"
     ]
    }
   ],
   "source": [
    "# want 5/7 * weekday = 1 + 2/7 * weekend = 0\n",
    "weekday_weekend_scaled_predictions = all_predictions_df[['Station Name', 'Weekday', 'Unscaled_Predicted_Total_Demand']].groupby('Station Name').apply(lambda x: 5/7 * x[x['Weekday'] == 1]['Unscaled_Predicted_Total_Demand'].values[0] + 2/7 * x[x['Weekday'] == 0]['Unscaled_Predicted_Total_Demand'].values[0]).reset_index().rename({0: 'Predicted_Total_Demand'}, axis=1).loc[:218]\n",
    "weekday_weekend_scaled_predictions.to_csv('../output/agnn2_predictions_weekday_weekend_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last 2 tuples correspond to weights for: 'Nearby Train Demand Aggregate', 'Weekday', 'PublicHoliday', 'mean_rainfall_value', 'has_school',\n",
    "       'has_sport_facility', 'has_shopping_centre', 'has_hospital',\n",
    "       'total_population', ' med_rent_weekly_c2021',\n",
    "       ' med_mortg_rep_mon_c2021', ' med_person_inc_we_c2021',\n",
    "       ' med_famly_inc_we_c2021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2207,  0.2396, -0.0676,  0.2652, -0.0632,  0.0583, -0.1405,  0.1695,\n",
      "          0.2545, -0.2118,  0.2509,  0.0540]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2133], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# A_GNN 2 layers\n",
    "class GNN_config:\n",
    "    # ----------------- architectual hyperparameters ----------------- #\n",
    "    d_model = 256\n",
    "    n_heads = 8\n",
    "    dropout = 0.1\n",
    "    n_gnn_layers = 2\n",
    "    activation = nn.ReLU()\n",
    "    res_learning = False\n",
    "    bottleneck = True\n",
    "    # ----------------- optimisation hyperparameters ----------------- #\n",
    "    random_state = SEED\n",
    "    epochs = 32\n",
    "    lr = 1e-3\n",
    "    patience = 5\n",
    "    loss = nn.MSELoss()\n",
    "    validation_loss = nn.MSELoss()\n",
    "    alpha = 0.1\n",
    "    scheduler = True\n",
    "    grad_clip = False\n",
    "    # ----------------- operation hyperparameters ----------------- #\n",
    "    spatial_input_dim = 1\n",
    "    nonspatial_input_dim = 11\n",
    "    # ----------------- saving hyperparameters ----------------- #\n",
    "    rootpath = home_directory\n",
    "    name = f'AGNN_2layer'\n",
    "\n",
    "model2 = GNN(GNN_config) # initialise the model\n",
    "\n",
    "params = list(model2.model.parameters())\n",
    "\n",
    "for param in params[-2:]:\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spanalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
