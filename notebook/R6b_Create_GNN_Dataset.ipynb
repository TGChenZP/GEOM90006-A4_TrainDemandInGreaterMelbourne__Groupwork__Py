{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create instances with all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trains_demand_pre_covid = pd.read_csv('../data/curated/train_demand/daily_trains_demand_pre_covid.csv')\n",
    "daily_trains_demand_post_covid = pd.read_csv('../data/curated/train_demand/daily_trains_demand_post_covid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trains_demand_post_covid_weekday = daily_trains_demand_post_covid[(daily_trains_demand_post_covid['Weekday'] == 1)]\n",
    "daily_trains_demand_post_covid_weekend = daily_trains_demand_post_covid[(daily_trains_demand_post_covid['Weekday'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rows together\n",
    "mean_daily_trains_demand_post_covid_weekday = pd.DataFrame()\n",
    "\n",
    "for id, station_data in daily_trains_demand_post_covid_weekday.drop(['Business_Date', 'Weekday', 'PublicHoliday', 'Unnamed: 0'], axis=1).groupby('Station_Name'):\n",
    "    station_data = station_data.drop('Station_Name', axis=1)\n",
    "    station_data_mean = station_data.mean()\n",
    "    station_data_mean = pd.DataFrame(station_data_mean).T\n",
    "    station_data_mean['Station_Name'] = id\n",
    "    mean_daily_trains_demand_post_covid_weekday = pd.concat([mean_daily_trains_demand_post_covid_weekday, station_data_mean], axis=0)\n",
    "\n",
    "\n",
    "mean_daily_trains_demand_post_covid_weekend = pd.DataFrame()\n",
    "\n",
    "for id, station_data in daily_trains_demand_post_covid_weekend.drop(['Business_Date', 'Weekday', 'PublicHoliday', 'Unnamed: 0'], axis=1).groupby('Station_Name'):\n",
    "    station_data = station_data.drop('Station_Name', axis=1)\n",
    "    station_data_mean = station_data.mean()\n",
    "    station_data_mean = pd.DataFrame(station_data_mean).T\n",
    "    station_data_mean['Station_Name'] = id\n",
    "    mean_daily_trains_demand_post_covid_weekend = pd.concat([mean_daily_trains_demand_post_covid_weekend, station_data_mean], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_daily_trains_demand_post_covid_weekday['log_Total_Demand'] = np.log(mean_daily_trains_demand_post_covid_weekday['Total_Demand'])\n",
    "mean_daily_trains_demand_post_covid_weekend['log_Total_Demand'] = np.log(mean_daily_trains_demand_post_covid_weekend['Total_Demand'])\n",
    "mean_daily_trains_demand_post_covid_weekday['log_Passenger_Alightings'] = np.log(mean_daily_trains_demand_post_covid_weekday['Passenger_Alightings'])\n",
    "mean_daily_trains_demand_post_covid_weekend['log_Passenger_Alightings'] = np.log(mean_daily_trains_demand_post_covid_weekend['Passenger_Alightings'])\n",
    "mean_daily_trains_demand_post_covid_weekday['log_Passenger_Boardings'] = np.log(mean_daily_trains_demand_post_covid_weekday['Passenger_Boardings'])\n",
    "mean_daily_trains_demand_post_covid_weekend['log_Passenger_Boardings'] = np.log(mean_daily_trains_demand_post_covid_weekend['Passenger_Boardings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_demand_in_rows(demand_df: pd.DataFrame, feature: str):\n",
    "\n",
    "    try:\n",
    "        df_with_feature = demand_df[['Station_Name', 'Business_Date', feature]]\n",
    "    except:\n",
    "        df_with_feature = demand_df[['Station_Name', feature]]\n",
    "\n",
    "    station_df_list = []\n",
    "\n",
    "    for id, station_df in tqdm(df_with_feature.groupby('Station_Name')):\n",
    "\n",
    "        station_df = station_df.rename({feature: f'{feature}_{id}'}, axis=1)\n",
    "        station_df = station_df.drop('Station_Name', axis=1)\n",
    "        station_df_list.append(station_df)\n",
    "\n",
    "    for i, station_df in enumerate(station_df_list):\n",
    "        if i == 0:\n",
    "            merged_df = station_df\n",
    "        else:\n",
    "            try:\n",
    "                merged_df = pd.merge(merged_df, station_df, on='Business_Date', how='outer')\n",
    "            except:\n",
    "                merged_df = pd.concat([merged_df, station_df], axis=0)\n",
    "\n",
    "    merged_df = merged_df.fillna(0)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [00:00<00:00, 2289.11it/s]\n",
      "100%|██████████| 223/223 [00:00<00:00, 2556.96it/s]\n",
      "100%|██████████| 222/222 [00:00<00:00, 2527.64it/s]\n",
      "100%|██████████| 223/223 [00:00<00:00, 2573.19it/s]\n",
      "100%|██████████| 222/222 [00:00<00:00, 2620.83it/s]\n",
      "100%|██████████| 223/223 [00:00<00:00, 1718.73it/s]\n"
     ]
    }
   ],
   "source": [
    "log_demand_precovid = get_daily_demand_in_rows(daily_trains_demand_pre_covid, 'log_Total_Demand')\n",
    "log_demand_postcovid = get_daily_demand_in_rows(daily_trains_demand_post_covid, 'log_Total_Demand')\n",
    "log_alighting_precovid = get_daily_demand_in_rows(daily_trains_demand_pre_covid, 'log_Passenger_Alightings')\n",
    "log_alighting_postcovid = get_daily_demand_in_rows(daily_trains_demand_post_covid, 'log_Passenger_Alightings')\n",
    "log_boarding_precovid = get_daily_demand_in_rows(daily_trains_demand_pre_covid, 'log_Passenger_Boardings')\n",
    "log_boarding_postcovid = get_daily_demand_in_rows(daily_trains_demand_post_covid, 'log_Passenger_Boardings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_daily_trains_demand_post_covid_weekday = mean_daily_trains_demand_post_covid_weekday.sort_values('Station_Name')\n",
    "mean_daily_trains_demand_post_covid_weekday = mean_daily_trains_demand_post_covid_weekday.set_index('Station_Name')\n",
    "\n",
    "mean_daily_trains_demand_post_covid_weekend = mean_daily_trains_demand_post_covid_weekend.sort_values('Station_Name')\n",
    "mean_daily_trains_demand_post_covid_weekend = mean_daily_trains_demand_post_covid_weekend.set_index('Station_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_demand_in_rows_inference(df, feature):\n",
    "    return df[[feature]].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_mean_demand_postcovid_weekday = get_daily_demand_in_rows_inference(mean_daily_trains_demand_post_covid_weekday, 'log_Total_Demand')\n",
    "log_mean_alighting_postcovid_weekday = get_daily_demand_in_rows_inference(mean_daily_trains_demand_post_covid_weekday, 'log_Passenger_Alightings')\n",
    "log_mean_boarding_postcovid_weekday = get_daily_demand_in_rows_inference(mean_daily_trains_demand_post_covid_weekday, 'log_Passenger_Boardings')\n",
    "\n",
    "log_mean_demand_postcovid_weekend = get_daily_demand_in_rows_inference(mean_daily_trains_demand_post_covid_weekend, 'log_Total_Demand')\n",
    "log_mean_alighting_postcovid_weekend = get_daily_demand_in_rows_inference(mean_daily_trains_demand_post_covid_weekend, 'log_Passenger_Alightings')\n",
    "log_mean_boarding_postcovid_weekend = get_daily_demand_in_rows_inference(mean_daily_trains_demand_post_covid_weekend, 'log_Passenger_Boardings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../data/curated/ML_features', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_demand_precovid.to_csv('../data/curated/ML_features/log_demand_precovid.csv', index=False)\n",
    "# log_demand_postcovid.to_csv('../data/curated/ML_features/log_demand_postcovid.csv', index=False)\n",
    "# log_alighting_precovid.to_csv('../data/curated/ML_features/log_alighting_precovid.csv', index=False)\n",
    "# log_alighting_postcovid.to_csv('../data/curated/ML_features/log_alighting_postcovid.csv', index=False)\n",
    "# log_boarding_precovid.to_csv('../data/curated/ML_features/log_boarding_precovid.csv', index=False)\n",
    "# log_boarding_postcovid.to_csv('../data/curated/ML_features/log_boarding_postcovid.csv', index=False)\n",
    "\n",
    "# log_mean_demand_postcovid_weekday.to_csv('../data/curated/ML_features/log_mean_demand_postcovid_weekday.csv', index=False)\n",
    "# log_mean_alighting_postcovid_weekday.to_csv('../data/curated/ML_features/log_mean_alighting_postcovid_weekday.csv', index=False)\n",
    "# log_mean_boarding_postcovid_weekday.to_csv('../data/curated/ML_features/log_mean_boarding_postcovid_weekday.csv', index=False)\n",
    "# log_mean_alighting_postcovid_weekend.to_csv('../data/curated/ML_features/log_mean_alighting_postcovid_weekend.csv', index=False)\n",
    "# log_mean_boarding_postcovid_weekend.to_csv('../data/curated/ML_features/log_mean_boarding_postcovid_weekend.csv', index=False)\n",
    "# log_mean_demand_postcovid_weekend.to_csv('../data/curated/ML_features/log_mean_demand_postcovid_weekend.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_weights_withSA2 = pd.read_csv('../data/curated/ML_features/station_weights_withSA2.csv')\n",
    "station_weights = pd.read_csv('../data/curated/ML_features/station_weights.csv')\n",
    "\n",
    "station_with_sa2_list_dict = {k:i for i, k in enumerate(station_weights_withSA2['Unnamed: 0'])}\n",
    "station_list_dict = {k:i for i, k in enumerate(station_weights['Unnamed: 0'])}\n",
    "reverse_station_with_sa2_list_dict = {i:k for i, k in enumerate(station_weights_withSA2['Unnamed: 0'])}\n",
    "reverse_station_list_dict = {i:k for i, k in enumerate(station_weights['Unnamed: 0'])}\n",
    "\n",
    "station_weights_withSA2.set_index('Unnamed: 0', inplace=True)\n",
    "station_weights.set_index('Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this numpy\n",
    "np.save('../data/curated/ML_data/station_weights_matrix.npy', station_weights.values)\n",
    "np.save('../data/curated/ML_data/station_weights_withSA2_matrix.npy', station_weights_withSA2.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trains_demand_post_covid.drop(['Unnamed: 0', 'Passenger_Boardings', 'Passenger_Alightings', 'Total_Demand', 'log_Passenger_Boardings', 'log_Passenger_Alightings'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall_df = pd.read_csv('../data/curated/ML_features/rainfall_Station_SA2.csv')\n",
    "rainfall_df_stations = rainfall_df[~rainfall_df['Station_Na'].isna()][['mean_rainfall_value', 'Station_Na']]\n",
    "rainfall_df_sa2 = rainfall_df[rainfall_df['Station_Na'].isna()][['mean_rainfall_value', 'SA2_NAME21']]\n",
    "rainfall_df_sa2.rename({'SA2_NAME21': 'Station'}, axis=1, inplace=True)\n",
    "\n",
    "census_and_buildings_postcovid = pd.read_csv('../data/curated/ML_features/census_and_buildings_postcovid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ML_data = daily_trains_demand_post_covid.merge(rainfall_df_stations, left_on='Station_Name', right_on='Station_Na', how='left')\n",
    "station_ML_data.drop(['Station_Na'], axis=1, inplace=True)\n",
    "station_ML_data = station_ML_data.merge(census_and_buildings_postcovid[census_and_buildings_postcovid['point_type'] == 'station'], left_on = 'Station_Name', right_on='Point Name', how = 'left')\n",
    "\n",
    "station_ML_data.drop(['Point Name', 'point_type'], inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inference mean data\n",
    "station_ML_data_weekday = rainfall_df_stations.copy()\n",
    "station_ML_data_weekday = station_ML_data_weekday.merge(log_mean_demand_postcovid_weekday.T.reset_index(), left_on='Station_Na', right_on='Station_Name', how='left')\n",
    "station_ML_data_weekday = station_ML_data_weekday.merge(census_and_buildings_postcovid[census_and_buildings_postcovid['point_type'] == 'station'], left_on = 'Station_Na', right_on='Point Name', how = 'left')\n",
    "\n",
    "station_ML_data_weekday.drop(['Point Name', 'point_type'], axis=1, inplace=True)\n",
    "station_ML_data_weekday.rename({'Station': 'Station Name'}, axis=1, inplace=True)\n",
    "\n",
    "station_ML_data_weekend = rainfall_df_stations.copy()\n",
    "station_ML_data_weekend = station_ML_data_weekend.merge(log_mean_demand_postcovid_weekend.T.reset_index(), left_on='Station_Na', right_on='Station_Name', how='left')\n",
    "station_ML_data_weekend = station_ML_data_weekend.merge(census_and_buildings_postcovid[census_and_buildings_postcovid['point_type'] == 'station'], left_on = 'Station_Na', right_on='Point Name', how = 'left')\n",
    "\n",
    "station_ML_data_weekend.drop(['Point Name', 'point_type'], axis=1, inplace=True)\n",
    "station_ML_data_weekend.rename({'Station': 'Station Name'}, axis=1, inplace=True)\n",
    "\n",
    "station_ML_data_weekday['Weekday'] = 1\n",
    "station_ML_data_weekend['Weekday'] = 0\n",
    "\n",
    "station_inference_ML_data = pd.concat([station_ML_data_weekday, station_ML_data_weekend], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inference SA2 data\n",
    "SA2_ML_data_weekday = rainfall_df_sa2.copy()\n",
    "SA2_ML_data_weekday = SA2_ML_data_weekday.merge(census_and_buildings_postcovid[census_and_buildings_postcovid['point_type'] == 'suburb'], left_on = 'Station', right_on='Point Name', how = 'left')\n",
    "\n",
    "SA2_ML_data_weekday.drop(['Point Name', 'point_type'], axis=1, inplace=True)\n",
    "SA2_ML_data_weekday.rename({'Station': 'Station Name'}, axis=1, inplace=True)\n",
    "\n",
    "SA2_ML_data_weekend = rainfall_df_sa2.copy()\n",
    "SA2_ML_data_weekend = SA2_ML_data_weekend.merge(census_and_buildings_postcovid[census_and_buildings_postcovid['point_type'] == 'suburb'], left_on = 'Station', right_on='Point Name', how = 'left')\n",
    "\n",
    "SA2_ML_data_weekend.drop(['Point Name', 'point_type'], axis=1, inplace=True)\n",
    "SA2_ML_data_weekend.rename({'Station': 'Station Name'}, axis=1, inplace=True)\n",
    "\n",
    "SA2_ML_data_weekday['Weekday'] = 1\n",
    "SA2_ML_data_weekend['Weekday'] = 0\n",
    "\n",
    "SA2_ML_data = pd.concat([SA2_ML_data_weekday, SA2_ML_data_weekend], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../data/curated/ML_data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ML_data.to_parquet('../data/curated/ML_data/station_gnn_data.parquet', index=False)\n",
    "SA2_ML_data.to_parquet('../data/curated/ML_data/SA2_gnn_data.parquet', index=False)\n",
    "station_inference_ML_data.to_parquet('../data/curated/ML_data/station_inference_gnn_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split, Feature Selection and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "station_ML_data = pd.read_parquet('../data/curated/ML_data/station_gnn_data.parquet')\n",
    "SA2_ML_data = pd.read_parquet('../data/curated/ML_data/SA2_gnn_data.parquet')\n",
    "station_inference_ML_data = pd.read_parquet('../data/curated/ML_data/station_inference_gnn_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    " 'average_hh_size_c2021',\n",
    " ' med_tot_hh_inc_wee_c2021',\n",
    " ' avg_num_p_per_brm_c2021',\n",
    " ' med_age_persns_c2021',]\n",
    "for column in station_ML_data.columns:\n",
    "    if 'boarding' in column or 'alighting' in column:\n",
    "        drop_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ML_data = station_ML_data.drop(drop_columns, axis = 1)\n",
    "SA2_ML_data = SA2_ML_data.drop(drop_columns, axis = 1)\n",
    "station_inference_ML_data = station_inference_ML_data.drop(drop_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_dates = station_ML_data['Business_Date'].unique()\n",
    "\n",
    "train_dates, val_test_dates = train_test_split(business_dates, test_size=0.3, shuffle = False)\n",
    "val_dates, test_dates = train_test_split(val_test_dates, test_size=0.5, shuffle = False)\n",
    "\n",
    "ML_train_data = station_ML_data[station_ML_data['Business_Date'].isin(train_dates)]\n",
    "ML_val_data = station_ML_data[station_ML_data['Business_Date'].isin(val_dates)]\n",
    "ML_test_data = station_ML_data[station_ML_data['Business_Date'].isin(test_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [col for col in station_ML_data if col not in  ['Station_Name',\n",
    "                                                                    'Business_Date',\n",
    "                                                                    'Weekday',\n",
    "                                                                    'PublicHoliday',\n",
    "                                                                    'has_school', \n",
    "                                                                    'has_sport_facility',\n",
    "                                                                    'has_shopping_centre',\n",
    "                                                                    'has_hospital', \n",
    "                                                                    'Date',\n",
    "                                                                    'log_Total_Demand']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_15876/3450036050.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ML_train_data[numerical_columns] = scaler.fit_transform(ML_train_data[numerical_columns])\n",
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_15876/3450036050.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ML_val_data[numerical_columns] = scaler.transform(ML_val_data[numerical_columns])\n",
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_15876/3450036050.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ML_test_data[numerical_columns] = scaler.transform(ML_test_data[numerical_columns])\n",
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_15876/3450036050.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ML_train_data[['log_Total_Demand']] = y_scaler.fit_transform(ML_train_data[['log_Total_Demand']])\n",
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_15876/3450036050.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ML_val_data[['log_Total_Demand']] = y_scaler.transform(ML_val_data[['log_Total_Demand']])\n",
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_15876/3450036050.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ML_test_data[['log_Total_Demand']] = y_scaler.transform(ML_test_data[['log_Total_Demand']])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical columns\n",
    "ML_train_data[numerical_columns] = scaler.fit_transform(ML_train_data[numerical_columns])\n",
    "ML_val_data[numerical_columns] = scaler.transform(ML_val_data[numerical_columns])\n",
    "ML_test_data[numerical_columns] = scaler.transform(ML_test_data[numerical_columns])\n",
    "\n",
    "ML_train_data[['log_Total_Demand']] = y_scaler.fit_transform(ML_train_data[['log_Total_Demand']])\n",
    "ML_val_data[['log_Total_Demand']] = y_scaler.transform(ML_val_data[['log_Total_Demand']])\n",
    "ML_test_data[['log_Total_Demand']] = y_scaler.transform(ML_test_data[['log_Total_Demand']])\n",
    "\n",
    "SA2_ML_data[numerical_columns] = scaler.transform(SA2_ML_data[numerical_columns])\n",
    "station_inference_ML_data[numerical_columns] = scaler.transform(station_inference_ML_data[numerical_columns])\n",
    "\n",
    "with open('../data/curated/ML_data/scaler_gnn.pickle', 'wb') as f:\n",
    "    pickle.dump(scaler, f) \n",
    "\n",
    "with open('../data/curated/ML_data/y_scaler_gnn.pickle', 'wb') as f:\n",
    "    pickle.dump(y_scaler, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_train_data.to_parquet('../data/curated/ML_data/gnn_train_data.parquet', index=False)\n",
    "ML_val_data.to_parquet('../data/curated/ML_data/gnn_val_data.parquet', index=False)\n",
    "ML_test_data.to_parquet('../data/curated/ML_data/gnn_test_data.parquet', index=False)\n",
    "\n",
    "station_inference_ML_data.to_parquet('../data/curated/ML_data/station_inference_gnn_data.parquet', index=False)\n",
    "SA2_ML_data.to_parquet('../data/curated/ML_data/SA2_gnn_data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spanalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
